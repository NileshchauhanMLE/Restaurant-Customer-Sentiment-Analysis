{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##This code strips all the meta data from one restaurant and stores the information into a dictionary called rest_dict\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup \n",
    "import os.path\n",
    "import csv\n",
    "\n",
    "#grabs the location of all restaurant html files on the local hard drive\n",
    "import glob\n",
    "\n",
    "all_restaurants_html = []\n",
    "\n",
    "for filename in glob.iglob(r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_Webpages\\*page_endson_0.html'):\n",
    "    all_restaurants_html.append(filename)\n",
    "\n",
    "for url in all_restaurants_html:\n",
    "    #retrieve the html pages from your hard drive and then create a for loop that will go through each URL\n",
    "    #and extract and compile the desired Xpaths\n",
    "    tree = html.parse(url)\n",
    "\n",
    "    #variable restaurant more business info\n",
    "    with open(url,'rb') as file:\n",
    "        soup = BeautifulSoup(file)\n",
    "\n",
    "    attributes = []\n",
    "    for element in soup.find_all('div',attrs={'class':'short-def-list'}):\n",
    "        for subelements in element.find_all('dt',attrs={'class':'attribute-key'}):\n",
    "            text = subelements.get_text()\n",
    "            attributes.append(text.strip())\n",
    "\n",
    "    values = []\n",
    "    for element in soup.find_all('div',attrs={'class':'short-def-list'}):\n",
    "        for subelements in element.find_all('dd'):\n",
    "            text = subelements.get_text()\n",
    "            values.append(text.strip())\n",
    "\n",
    "    #create a dictionary and add the variable business information - (eg restaurant hours, bar service, ambiance)\n",
    "    dictionary = dict(zip(attributes, values))\n",
    "    \n",
    "    #create a separate dictionary for the cuisines part of the restaurant. multiple cuisines requires a loop\n",
    "    cuisine = []\n",
    "    for element in soup.find('div',attrs={'class':'price-category'}).find_all('span',attrs={'class':'category-str-list'}):\n",
    "        for subelements in element.find_all('a'):\n",
    "            text = subelements.get_text()\n",
    "            cuisine.append(text)\n",
    "            \n",
    "    #extract standard business information - (eg name, rating, # of reviews, location)\n",
    "    name = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[1]/h1/text()')\n",
    "    rating = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[2]/div[1]/div[1]/div/@title')\n",
    "    count = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[2]/div[1]/div[1]/span/text()')\n",
    "    cost = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[2]/div[2]/span[1]/span/text()')\n",
    "    price_range = tree.xpath('//*[@id=\"super-container\"]/div/div/div[2]/div[1]/div[2]/ul/li[3]/div[2]/dl/dd/text()')\n",
    "    address = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[4]/div[1]/div/div[2]/ul/li[1]/div/strong/address/text()')\n",
    "\n",
    "    #add standard business information to existing dictionary\n",
    "    dictionary['Restaurant Name'] = ''.join(name).strip()\n",
    "    dictionary['Rating'] = ''.join(rating).strip()\n",
    "    dictionary['Review Count'] = ''.join(count).strip()\n",
    "    dictionary['Expected Cost'] = ''.join(cost).strip()\n",
    "    dictionary['Price Range'] = ''.join(price_range).strip()\n",
    "    dictionary['Address'] = ''.join(address).strip()\n",
    "    dictionary['Cuisines'] = cuisine\n",
    "    \n",
    "    #create a variable for the restaurant name to be included in the file name\n",
    "    restaurant_name =  dictionary['Restaurant Name']\n",
    "    \n",
    "    #remove characters that prevent one from creating a unique file name\n",
    "    restaurant_name = restaurant_name.replace(\"?\", \"\")\n",
    "    \n",
    "    #save the dictionary for each restaurnat into a csv file \n",
    "    save_path = r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_ParsedData\\metadata'\n",
    "    completeName = os.path.join(save_path, restaurant_name + '.csv')\n",
    "    with open(completeName,'w') as handle:\n",
    "        w = csv.DictWriter(handle,dictionary.keys())\n",
    "        w.writeheader()\n",
    "        w.writerow(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converts all csv files saved to hard drive to a pandas data frame for testing\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "#locate the meta data for restaurants extracted from the html files\n",
    "path = r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_ParsedData\\metadata'\n",
    "filenames = glob.glob(os.path.join(path + '\\*.csv'))\n",
    "\n",
    "#extract the encoding information for each csv file \n",
    "enc = []\n",
    "for file in filenames:\n",
    "    opened_text = open(file,'rb')\n",
    "    result = chardet.detect(opened_text.read())\n",
    "    enc.append(result)\n",
    "\n",
    "#read the file \n",
    "dataframe_per_file = (pd.read_csv(f, encoding = enc[filenames.index(f)]['encoding']) for f in filenames)\n",
    "\n",
    "concatenated_df = pd.concat(dataframe_per_file, ignore_index = True)\n",
    "concatenated_df = concatenated_df[['Restaurant Name', 'Rating', 'Review Count', 'Expected Cost', 'Price Range', 'Cuisines', 'Accepts Credit Cards', 'Address', 'Alcohol', 'Ambience', 'Attire', 'Bike Parking', 'Caters',  'Delivery', 'Good For', 'Good for Groups', 'Good for Kids', 'Has TV', 'Noise Level', 'Outdoor Seating','Parking', 'Take-out', 'Takes Reservations', 'Waiter Service', 'Wheelchair Accessible', 'Wi-Fi']]\n",
    "print(concatenated_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#maximum number of reviews that a Thai restaurant in NYC has ever received\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "#safety variable\n",
    "rest_df = concatenated_df\n",
    "\n",
    "temp_list1 = []\n",
    "regex = (r'\\d+')\n",
    "for x in rest_df['Review Count']:\n",
    "    x = str(x)\n",
    "    if re.search(regex, x):\n",
    "        match = re.search(regex,x)\n",
    "        temp_list1.append(int(match.group(0)))\n",
    "    else:\n",
    "        temp_list1.append(np.nan)\n",
    "\n",
    "rest_df['Review Count Num'] = temp_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cleaning the number of rating of each restaurant\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "regex = r'(\\d+\\.\\d+)'\n",
    "\n",
    "list = []\n",
    "for x in rest_df['Rating']:\n",
    "    if re.search(regex, str(x)):\n",
    "        match = re.search(regex, x)\n",
    "        list.append(float(match.group(0)))\n",
    "    else:\n",
    "        list.append(np.nan)\n",
    "    \n",
    "rest_df['Clean Rating'] = list\n",
    "\n",
    "rest_df.to_csv('yelp_metadata_v0_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##This code strips all the review data from one restaurant and stores the information into a dictionary called rest_dict\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup \n",
    "import os.path\n",
    "import csv\n",
    "\n",
    "#grabs the location of all restaurant html files on the local hard drive\n",
    "import glob\n",
    "\n",
    "all_restaurants_html = []\n",
    "\n",
    "for filename in glob.iglob(r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_Webpages\\*.html'):\n",
    "    all_restaurants_html.append(filename)\n",
    "\n",
    "for url in all_restaurants_html:\n",
    "    #retrieve the html pages from your hard drive and then create a for loop that will go through each URL\n",
    "    #and extract and compile the desired Xpaths\n",
    "    tree = html.parse(url)\n",
    "\n",
    "    #variable restaurant more business info\n",
    "    with open(url,'rb') as file:\n",
    "        soup = BeautifulSoup(file)\n",
    "\n",
    "    attributes = []\n",
    "    for element in soup.find_all('div',attrs={'class':'short-def-list'}):\n",
    "        for subelements in element.find_all('dt',attrs={'class':'attribute-key'}):\n",
    "            text = subelements.get_text()\n",
    "            attributes.append(text.strip())\n",
    "\n",
    "    values = []\n",
    "    for element in soup.find_all('div',attrs={'class':'short-def-list'}):\n",
    "        for subelements in element.find_all('dd'):\n",
    "            text = subelements.get_text()\n",
    "            values.append(text.strip())\n",
    "\n",
    "    #create a dictionary and add the variable business information - (eg restaurant hours, bar service, ambiance)\n",
    "    dictionary = dict(zip(attributes, values))\n",
    "    \n",
    "    #create a separate dictionary for the cuisines part of the restaurant. multiple cuisines requires a loop\n",
    "    cuisine = []\n",
    "    for element in soup.find('div',attrs={'class':'price-category'}).find_all('span',attrs={'class':'category-str-list'}):\n",
    "        for subelements in element.find_all('a'):\n",
    "            text = subelements.get_text()\n",
    "            cuisine.append(text)\n",
    "            \n",
    "    #extract standard business information - (eg name, rating, # of reviews, location)\n",
    "    name = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[1]/h1/text()')\n",
    "    rating = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[2]/div[1]/div[1]/div/@title')\n",
    "    count = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[2]/div[1]/div[1]/span/text()')\n",
    "    cost = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[2]/div[2]/span[1]/span/text()')\n",
    "    price_range = tree.xpath('//*[@id=\"super-container\"]/div/div/div[2]/div[1]/div[2]/ul/li[3]/div[2]/dl/dd/text()')\n",
    "    address = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[4]/div[1]/div/div[2]/ul/li[1]/div/strong/address/text()')\n",
    "\n",
    "    #add standard business information to existing dictionary\n",
    "    dictionary['Restaurant Name'] = ''.join(name).strip()\n",
    "    dictionary['Rating'] = ''.join(rating).strip()\n",
    "    dictionary['Review Count'] = ''.join(count).strip()\n",
    "    dictionary['Expected Cost'] = ''.join(cost).strip()\n",
    "    dictionary['Price Range'] = ''.join(price_range).strip()\n",
    "    dictionary['Address'] = ''.join(address).strip()\n",
    "    dictionary['Cuisines'] = cuisine\n",
    "    \n",
    "    #create a variable for the restaurant name to be included in the file name\n",
    "    restaurant_name =  dictionary['Restaurant Name']\n",
    "    \n",
    "    #remove characters that prevent one from creating a unique file name\n",
    "    restaurant_name = restaurant_name.replace(\"?\", \"\")\n",
    "    \n",
    "    #save the dictionary for each restaurnat into a csv file \n",
    "    save_path = r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_ParsedData\\metadata'\n",
    "    completeName = os.path.join(save_path, restaurant_name + '.csv')\n",
    "    with open(completeName,'w') as handle:\n",
    "        w = csv.DictWriter(handle,dictionary.keys())\n",
    "        w.writeheader()\n",
    "        w.writerow(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##This code strips all the review data from one restaurant and stores the information into a dictionary called rest_dict\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup \n",
    "import os.path\n",
    "import csv\n",
    "import re\n",
    "\n",
    "all_restaurants_html = []\n",
    "\n",
    "for filename in glob.iglob(r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_Webpages\\*page_endson_0.html'):\n",
    "    all_restaurants_html.append(filename)\n",
    "\n",
    "all_restaurants = []\n",
    "for uncleaned_restaurant in all_restaurants_html[17:]:\n",
    "    cleaned_restaurant_html = uncleaned_restaurant.split('_page_endson', 1)[0]\n",
    "    all_restaurants.append(cleaned_restaurant_html)\n",
    "    \n",
    "#for loop to iterate across all restaraunts - we will iterate across a list of all resturants\n",
    "for restaurant in all_restaurants:\n",
    "    \n",
    "    restaurant_review_num = 1\n",
    "    restaurant_query = restaurant + '*'\n",
    "    restaurant_review_pages = glob.iglob(restaurant_query)\n",
    "\n",
    "    for a_review_page in restaurant_review_pages:\n",
    "        #retrieve the html pages from your hard drive and then create a for loop that will go through each URL\n",
    "        #and extract and compile the desired Xpaths\n",
    "        tree = html.parse(a_review_page)\n",
    "\n",
    "        #variable restaurant more business info\n",
    "        with open(a_review_page,'rb') as file:\n",
    "            soup = BeautifulSoup(file)\n",
    "\n",
    "        for element in soup.find_all('div',attrs={'class':'review review--with-sidebar'}):\n",
    "\n",
    "            review_dictionary = {} \n",
    "\n",
    "            name = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[1]/h1/text()')\n",
    "            rating = tree.xpath('//*[@id=\"wrap\"]/div[3]/div/div[1]/div/div[3]/div[1]/div[2]/div[1]/div[1]/div/@title')\n",
    "            review_dictionary['Restaurant Name'] = ''.join(name).strip()\n",
    "            review_dictionary['Rating'] = ''.join(rating).strip()\n",
    "\n",
    "            \n",
    "            reviewer_raw = element.find('li',attrs={'class':'user-name'})\n",
    "            text = reviewer_raw.get_text()\n",
    "            reviewer = text.strip()\n",
    "\n",
    "            reviewer_num_reviews_raw = element.find('li',attrs={'class':'review-count responsive-small-display-inline-block'})\n",
    "            text = reviewer_num_reviews_raw.get_text()\n",
    "            reviewer_num_reviews = text.strip()\n",
    "\n",
    "            reviewer_elite_status_raw = element.find('li',attrs={'class':'is-elite responsive-small-display-inline-block'})\n",
    "            if reviewer_elite_status_raw != None:\n",
    "                text = reviewer_elite_status_raw.get_text()\n",
    "                reviewer_elite_status = text.strip()\n",
    "\n",
    "            #extracting reviewer_rating_raw and review_date_raw\n",
    "            reviewer_rating_raw = element.find('div', attrs={'class':'biz-rating biz-rating-very-large clearfix'}).find('div').find('div')['title']\n",
    "            reviewer_rating = reviewer_rating_raw.strip()\n",
    "\n",
    "            #extract the date of the review\n",
    "            review_date_raw = element.find('div', attrs={'class':'biz-rating biz-rating-very-large clearfix'}).get_text()\n",
    "            review_date = review_date_raw.strip()\n",
    "\n",
    "            review_raw = element.find('p')\n",
    "            text = review_raw.get_text()\n",
    "            review = text.strip()\n",
    "\n",
    "            review_dictionary['Reviewer'] = reviewer\n",
    "            review_dictionary['Reviewers Num of Review'] = reviewer_num_reviews\n",
    "            review_dictionary['Elite Status'] = reviewer_elite_status\n",
    "            review_dictionary['Reviewer Rating'] = reviewer_rating\n",
    "            review_dictionary['Review Date'] = review_date\n",
    "            review_dictionary['Review'] = review\n",
    "\n",
    "            #create a variable for the restaurant name to be included in the file name\n",
    "            restaurant_name =  review_dictionary['Restaurant Name']\n",
    "\n",
    "            #remove characters that prevent one from creating a unique file name\n",
    "            restaurant_name = restaurant_name.replace(\"?\", \"\")\n",
    "\n",
    "            #save the dictionary for each restaurnat into a csv file \n",
    "            save_path = r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_ParsedData\\data'\n",
    "            completeName = os.path.join(save_path, restaurant_name +'_review_num_' + str(restaurant_review_num) +'.csv')\n",
    "            with open(completeName,'w', encoding = 'utf-8') as handle:\n",
    "                w = csv.DictWriter(handle,review_dictionary.keys())\n",
    "                w.writeheader()\n",
    "                w.writerow(review_dictionary)\n",
    "\n",
    "            restaurant_review_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#converts all csv files saved to hard drive to a pandas data frame for testing\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "import chardet\n",
    "\n",
    "#locate the meta data for restaurants extracted from the html files\n",
    "path = r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_ParsedData\\data'\n",
    "filenames = glob.glob(os.path.join(path + '\\*.csv'))\n",
    "\n",
    "#extract the encoding information for each csv file \n",
    "enc = []\n",
    "for file in filenames:\n",
    "    opened_text = open(file,'rb')\n",
    "    result = chardet.detect(opened_text.read())\n",
    "    enc.append(result)\n",
    "\n",
    "#read the file \n",
    "dataframe_per_file2 = (pd.read_csv(f, encoding = enc[filenames.index(f)]['encoding']) for f in filenames)\n",
    "\n",
    "concatenated_df2 = pd.concat(dataframe_per_file2, ignore_index = True)\n",
    "#concatenated_df2 = concatenated_df[['Restaurant Name', 'Rating', 'Review Date', 'Reviewer Rating', 'Reviewers Num of Reviews', 'Reviewer', 'Elite Status']]\n",
    "print(concatenated_df2.head())\n",
    "concatenated_df2.to_csv('yelp_data_v0_1.csv')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
