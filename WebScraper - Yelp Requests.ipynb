{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olarn\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Olarn\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "import re\n",
    "allThairestaurantsNYC = []\n",
    "\n",
    "allThairestaurantsNYC_URL = 'https://www.yelp.com/search?find_desc=thai+restaurants&find_loc=New+York+City&cflt=thai'\n",
    "r = requests.get(allThairestaurantsNYC_URL)\n",
    "yelp_text = r.text\n",
    "soup = BeautifulSoup(yelp_text)\n",
    "\n",
    "#download the number of counts of restaurants in NYC from the yelp query\n",
    "restaurant_count = soup.find('span',{'class':'pagination-results-window'}).contents\n",
    "\n",
    "#use regular expression to extract the digits from the string that was extracted\n",
    "restaurant_count = re.findall('\\d+', str(restaurant_count))\n",
    "\n",
    "#select the third object in the list which represents the number of restaurants in NYC\n",
    "restaurant_count = int(restaurant_count[2])\n",
    "\n",
    "#total number of pages we will need to load. if we do not have an number of reviews divisible by 20, \n",
    "#we will need to review an additional page\n",
    "restaurant_pages = int(restaurant_count/10) \n",
    "if restaurant_count%10 > 0:\n",
    "    restaurant_pages += 1\n",
    "\n",
    "#creates a list of parameter inputs to load each page of the restaurant \n",
    "rest_params_variable = []\n",
    "for number in range(restaurant_pages):\n",
    "    number = (number)*10\n",
    "    rest_params_variable.append({'start' : number })\n",
    "\n",
    "#loops over each parameter and then apply beautiful soup so we can read the data on each restaurant page\n",
    "\n",
    "for param in rest_params_variable:\n",
    "    temp_r = requests.get(allThairestaurantsNYC_URL,params = param)\n",
    "    temp_text = temp_r.text\n",
    "    temp_soup = BeautifulSoup(temp_text)\n",
    "\n",
    "    for each_restaurant in temp_soup.select(\".regular-search-result\"):\n",
    "        href_link = each_restaurant.find('a')['href']\n",
    "        final_link = 'https://www.yelp.com' + href_link\n",
    "        allThairestaurantsNYC.append(final_link)\n",
    "\n",
    "#clean the URLs for all ThairestaurantsinNYC of any \"?search_key=\" jargon\n",
    "for x in allThairestaurantsNYC:\n",
    "    index = allThairestaurantsNYC.index(x)\n",
    "    mystring = x\n",
    "    keyword = '?'\n",
    "    before_keyword, keyword, after_keyword = mystring.partition(keyword)\n",
    "    allThairestaurantsNYC[index] = before_keyword\n",
    "        \n",
    "#################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olarn\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Olarn\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "for aThairestaurant in allThairestaurantsNYC: \n",
    "#download the reviews on the first page of a restaurant\n",
    "    restaurant_url = aThairestaurant\n",
    "    restaurant_name = restaurant_url.replace('https://www.yelp.com/biz/','')\n",
    "    \n",
    "    restaurant_reviews_raw = []\n",
    "    params_variable = []\n",
    "    html_page_name = []\n",
    "\n",
    "    r = requests.get(aThairestaurant)\n",
    "    yelp_text = r.text\n",
    "    soup = BeautifulSoup(yelp_text)\n",
    "\n",
    "    #extracts the number of reviews a restaurant has received\n",
    "    \n",
    "    if soup.find('span',{'class':'review-count rating-qualifier'}) == None:\n",
    "        continue\n",
    "    \n",
    "    reviews = soup.find('span',{'class':'review-count rating-qualifier'}).contents\n",
    "    reviews = str(reviews)\n",
    "    reviews = [int(s) for s in reviews.split() if s.isdigit()]\n",
    "    if not reviews:\n",
    "        continue\n",
    "    review_count = reviews[0]\n",
    "\n",
    "    #determines the total number of pages we will need to download per restaurant based on number of reviews\n",
    "    review_pages = int(review_count/20)\n",
    "    if review_count%20 > 0:\n",
    "        review_pages += 1\n",
    "\n",
    "    #download the reviews located on all other yelp pages\n",
    "    #create a list of request parameter inputs to feed into a FOR loop with requests. Each parameter represents another page.\n",
    "\n",
    "    #subtract by 1 because range starts at 0 instead of 1, and we would like there to be a total of 13 review pages, not 14\n",
    "    for number in range(review_pages):\n",
    "        number = (number)*20\n",
    "        html_page_name.append(str(number))\n",
    "        params_variable.append({'start' : number })\n",
    "\n",
    "    #feed the list into requests to download all webpages\n",
    "    for param in params_variable:\n",
    "        temp_r = requests.get(restaurant_url,params = param)\n",
    "        temp_text = temp_r.text\n",
    "        temp_soup = BeautifulSoup(temp_text)\n",
    "        restaurant_reviews_raw.append(temp_soup.encode(\"utf-8\"))\n",
    "\n",
    "    #save data to an independent html output file\n",
    "    save_path = r'C:\\Users\\Olarn\\Desktop\\Data Science\\Springboard\\Yelp_Webpages_2'\n",
    "\n",
    "    for item in restaurant_reviews_raw:\n",
    "        index = restaurant_reviews_raw.index(item)\n",
    "        new_path = restaurant_name +'_page_endson_'+ html_page_name[index] + '.html'\n",
    "        completeName = os.path.join(save_path, new_path)\n",
    "        thefile = open(completeName, 'wb')\n",
    "        thefile.write(item)\n",
    "        thefile.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
